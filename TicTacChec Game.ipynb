{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe498b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Define Tic Tac Chec constants\n",
    "BOARD_SIZE = 4\n",
    "NUM_PIECES = 8\n",
    "WINNING_CONDITION = 4\n",
    "NUM_ACTIONS = BOARD_SIZE**2 + NUM_PIECES\n",
    "\n",
    "# Define Tic Tac Chec environment\n",
    "class TicTacChecEnv:\n",
    "    def __init__(self):\n",
    "        self.board = [[' ' for _ in range(BOARD_SIZE)] for _ in range(BOARD_SIZE)]\n",
    "        self.current_player = 'White'\n",
    "        self.moves_left = BOARD_SIZE**2\n",
    "\n",
    "    def reset(self):\n",
    "        self.board = [[' ' for _ in range(BOARD_SIZE)] for _ in range(BOARD_SIZE)]\n",
    "        self.current_player = 'White'\n",
    "        self.moves_left = BOARD_SIZE**2\n",
    "        return self.get_state()\n",
    "\n",
    "    def get_state(self):\n",
    "        return np.array([[1 if cell == 'White' else -1 if cell == 'Black' else 0 for cell in row] for row in self.board])\n",
    "\n",
    "    def is_valid_move(self, row, col, piece):\n",
    "        return self.board[row][col] == ' ' and self.moves_left > 0\n",
    "\n",
    "    def make_move(self, row, col, piece):\n",
    "        if self.is_valid_move(row, col, piece):\n",
    "            self.board[row][col] = piece\n",
    "            self.moves_left -= 1\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def check_winner(self, player):\n",
    "        # Check rows, columns, and diagonals for a win\n",
    "        for i in range(BOARD_SIZE):\n",
    "            for j in range(BOARD_SIZE - WINNING_CONDITION + 1):\n",
    "                if all(self.board[i][j+k] == player for k in range(WINNING_CONDITION)) or \\\n",
    "                   all(self.board[j+k][i] == player for k in range(WINNING_CONDITION)):\n",
    "                    return True\n",
    "\n",
    "        for i in range(BOARD_SIZE - WINNING_CONDITION + 1):\n",
    "            for j in range(BOARD_SIZE - WINNING_CONDITION + 1):\n",
    "                if all(self.board[i+k][j+k] == player for k in range(WINNING_CONDITION)) or \\\n",
    "                   all(self.board[i+k][j+WINNING_CONDITION-1-k] == player for k in range(WINNING_CONDITION)):\n",
    "                    return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def is_draw(self):\n",
    "        return self.moves_left == 0\n",
    "\n",
    "    def switch_player(self):\n",
    "        self.current_player = 'Black' if self.current_player == 'White' else 'White'\n",
    "\n",
    "# Define Q-learning agent\n",
    "class QLearningAgent:\n",
    "    def __init__(self, num_states, num_actions, learning_rate=0.1, discount_factor=0.9, exploration_prob=0.2):\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.exploration_prob = exploration_prob\n",
    "        self.q_table = np.zeros((num_states, num_actions))\n",
    "        self.state = None\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        self.state = state\n",
    "        if random.uniform(0, 1) < self.exploration_prob:\n",
    "            return random.randint(0, self.num_actions - 1)\n",
    "        else:\n",
    "            return np.argmax(self.q_table[state])\n",
    "\n",
    "    def update_q_value(self, action, reward, next_state):\n",
    "        max_next_q_value = np.max(self.q_table[next_state])\n",
    "        self.q_table[self.state, action] += self.learning_rate * (\n",
    "            reward + self.discount_factor * max_next_q_value - self.q_table[self.state, action]\n",
    "        )\n",
    "\n",
    "# Main training loop\n",
    "def train_agent(agent, episodes):\n",
    "    for episode in range(episodes):\n",
    "        env = TicTacChecEnv()\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = agent.choose_action(state)\n",
    "            next_state = env.get_state()\n",
    "            reward = 0\n",
    "            if env.check_winner(1) or env.check_winner(-1):\n",
    "                done = True\n",
    "                if env.check_winner(1):\n",
    "                    reward = 1\n",
    "                else:\n",
    "                    reward = -1\n",
    "            elif env.is_draw():\n",
    "                done = True\n",
    "\n",
    "            agent.update_q_value(action, reward, next_state)\n",
    "            state = next_state\n",
    "\n",
    "# Initialize the Q-learning agent\n",
    "num_states = BOARD_SIZE**2\n",
    "num_actions = NUM_ACTIONS\n",
    "agent = QLearningAgent(num_states, num_actions)\n",
    "\n",
    "# Train the agent\n",
    "train_agent(agent, episodes=10000)\n",
    "\n",
    "# We can now use the trained agent to play the game or evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c43676e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(agent, env):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        if env.current_player == 'White':\n",
    "            # Agent's turn\n",
    "            action = agent.choose_action(state)\n",
    "            row, col, piece = action // BOARD_SIZE, action % BOARD_SIZE, 'White'\n",
    "            print(f\"Agent's move: {row}, {col}\")\n",
    "        else:\n",
    "            # Human's turn (you can replace this with user input)\n",
    "            print(\"Human's turn (row, col): \")\n",
    "            row, col = map(int, input().split())\n",
    "            piece = 'Black'\n",
    "\n",
    "        if env.is_valid_move(row, col, piece):\n",
    "            env.make_move(row, col, piece)\n",
    "\n",
    "        env.switch_player()\n",
    "        state = env.get_state()\n",
    "\n",
    "        if env.check_winner(1):\n",
    "            print(\"Agent wins!\")\n",
    "            done = True\n",
    "        elif env.check_winner(-1):\n",
    "            print(\"Human wins!\")\n",
    "            done = True\n",
    "        elif env.is_draw():\n",
    "            print(\"It's a draw!\")\n",
    "            done = True\n",
    "\n",
    "# Initialize the environment and agent\n",
    "env = TicTacChecEnv()\n",
    "agent = QLearningAgent(num_states, num_actions)\n",
    "\n",
    "# Load the trained Q-table (if available)\n",
    "# agent.q_table = np.load(\"q_table.npy\")\n",
    "\n",
    "# Play a game\n",
    "play_game(agent, env)\n",
    "\n",
    "# Save the Q-table (optional)\n",
    "# np.save(\"q_table.npy\", agent.q_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
